{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597983214577",
   "display_name": "Python 3.7.8 64-bit ('env_dgl_py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using backend: pytorch\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch as th\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create DGL graph, different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netwrokx object\n",
    "g_nx = nx.petersen_graph()\n",
    "g_dgl = dgl.DGLGraph(g_nx)\n",
    "\n",
    "plt.subplot(121)\n",
    "nx.draw(g_nx, with_labels=True)\n",
    "plt.subplot(122)\n",
    "nx.draw(g_dgl.to_networkx(), with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch tensors\n",
    "u = th.tensor([0])\n",
    "v = th.tensor([1, 2, 3, 4, 5])\n",
    "star1 = dgl.DGLGraph((u, v))\n",
    "# Visualize the graph.\n",
    "nx.draw(star1.to_networkx(), with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_toy = dgl.DGLGraph()\n",
    "g_toy.add_nodes(10)\n",
    "g_toy.add_edges(0,v.__reversed__())\n",
    "nx.draw(g_toy.to_networkx(), with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(g_toy.nodes())\n",
    "\n",
    "# assgin data values to nodes\n",
    "g_toy.ndata['pv'] = th.ones(N) / N\n",
    "g_toy.ndata['deg'] = g_toy.out_degrees(g_toy.nodes()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to send msg\n",
    "def pagerank_message_func(edges): # input is a `dgl.udf.EdgeBatch` class\n",
    "    return {'pv' : edges.src['pv'] / edges.src['deg']}\n",
    "\n",
    "# how you aggregate all received msg\n",
    "def pagerank_reduce_func(nodesBatch): # input is a `dgl.udf.NodeBatch` class, which has attribute mailbox\n",
    "    msgs = th.sum(nodesBatch.mailbox['pv'], dim=1)\n",
    "    pv = (1 - DAMP) / N + DAMP * msgs\n",
    "    return {'pv' : pv} # output is a dict\n",
    "\n",
    "DAMP = 0.5\n",
    "# register to this graph\n",
    "g_toy.register_message_func(pagerank_message_func)\n",
    "g_toy.register_reduce_func(pagerank_reduce_func)\n",
    "g_toy.ndata['pv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_toy.send()\n",
    "g_toy.recv()\n",
    "# now the `pv` values have been updated\n",
    "g_toy.ndata['pv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalently, use `update_all()` method\n",
    "# use dgl built-in functions to handle msg\n",
    "import dgl.function as fn\n",
    "def pagerank_builtin(g):\n",
    "    g.ndata['pv'] = g.ndata['pv'] / g.ndata['deg']\n",
    "    g.update_all(message_func=fn.copy_src(src='pv', out='m'),\n",
    "                 reduce_func=fn.sum(msg='m',out='m_sum'))\n",
    "    g.ndata['pv'] = (1 - DAMP) / N + DAMP * g.ndata['m_sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph Classification Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import MiniGCDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "# A dataset with 80 samples, each graph has [10, 20] number of nodes\n",
    "dataset = MiniGCDataset(80, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at specific graph example\n",
    "graph, label = dataset[33] # unpack the label\n",
    "fig, ax = plt.subplots()\n",
    "nx.draw(graph.to_networkx(), ax=ax)\n",
    "ax.set_title('Class: {:d}'.format(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collate function to combine multiple graphs into a batch\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples)) # similar to transpose\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, th.tensor(labels)\n",
    "\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
    "        # is the same as the out_degree.\n",
    "        h = g.in_degrees().view(-1, 1).float() # change dimension\n",
    "        # Perform graph convolution and activation function.\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        g.ndata['h'] = h\n",
    "        # Calculate graph representation by averaging all the node representations.\n",
    "        hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create training and test sets.\n",
    "trainset = MiniGCDataset(320, 10, 20)\n",
    "testset = MiniGCDataset(80, 10, 20)\n",
    "# Use PyTorch's DataLoader and the collate function defined before.\n",
    "data_loader = DataLoader(trainset, batch_size=32, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "\n",
    "# Create model\n",
    "model = Classifier(1, 256, trainset.num_classes)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(80):\n",
    "    epoch_loss = 0\n",
    "    for iter, (bg, label) in enumerate(data_loader):\n",
    "        prediction = model(bg)\n",
    "        loss = loss_func(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('cross entropy averaged over minibatches')\n",
    "plt.plot(epoch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}